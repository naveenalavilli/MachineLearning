{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
      "growing at a tremendous rate.  We are looking for individuals who\n",
      "want to work from home.\n",
      "\n",
      "This is an opportunity to make an excellent income.  No experience\n",
      "is required.  We will train you.\n",
      "\n",
      "So if you are looking to be employed from home with a career that has\n",
      "vast opportunities, then go:\n",
      "\n",
      "http://www.basetel.com/wealthnow\n",
      "\n",
      "We are looking for energetic and self motivated people.  If that is you\n",
      "than click on the link and fill out the form, and one of our\n",
      "employement specialist will contact you.\n",
      "\n",
      "To be removed from our link simple go to:\n",
      "\n",
      "http://www.basetel.com/remove.html\n",
      "\n",
      "\n",
      "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[6].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()\n",
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><pre>\n",
      "\n",
      "______________________________________________________________________\n",
      "______________________________________________________________________\n",
      "\n",
      "LOWEST RATES IN 45 YEARS -\n",
      "FILL OUT OUR SHORT APPLICATION FOR AN UNBELIEVABLE 3.50 - 5.0 % MORTGAGE\n",
      "(APR)\n",
      "\n",
      "  () HOME REFINANCING\n",
      "  () HOME IMPROVEMENT\n",
      "  () DEBT-CONSOLIDATION\n",
      "  () CASH-OUT\n",
      "\n",
      "Please Click <a href=\"http://210.192.106.2/mg/index.html\">HERE</a>\n",
      "for our short application.\n",
      "\n",
      "The following are NO problem and will not stop you from getting the\n",
      "financing you need:\n",
      "\n",
      "  *** Can't show income\n",
      "  *** Self-Employed\n",
      "  *** Credit Problems\n",
      "  *** Recent Bankruptcy\n",
      "  *** Unconventional Loan\n",
      "\n",
      "We are a direct lender and we have hundreds of loan programs available.\n",
      "If we don't have a program that works for you, we have hundreds of wholesale\n",
      "relationships with other lenders. So no matter which of our\n",
      "50 states you live in, we likely have a program that could meet your\n",
      "needs.\n",
      "\n",
      "Please Click <a href=\"http://210.192.106.2/mg/index.html\">HER ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "______________________________________________________________________\n",
      "______________________________________________________________________\n",
      "LOWEST RATES IN 45 YEARS -\n",
      "FILL OUT OUR SHORT APPLICATION FOR AN UNBELIEVABLE 3.50 - 5.0 % MORTGAGE\n",
      "(APR)\n",
      "  () HOME REFINANCING\n",
      "  () HOME IMPROVEMENT\n",
      "  () DEBT-CONSOLIDATION\n",
      "  () CASH-OUT\n",
      "Please Click  HYPERLINK HERE\n",
      "for our short application.\n",
      "The following are NO problem and will not stop you from getting the\n",
      "financing you need:\n",
      "  *** Can't show income\n",
      "  *** Self-Employed\n",
      "  *** Credit Problems\n",
      "  *** Recent Bankruptcy\n",
      "  *** Unconventional Loan\n",
      "We are a direct lender and we have hundreds of loan programs available.\n",
      "If we don't have a program that works for you, we have hundreds of wholesale\n",
      "relationships with other lenders. So no matter which of our\n",
      "50 states you live in, we likely have a program that could meet your\n",
      "needs.\n",
      "Please Click  HYPERLINK HERE\n",
      "for our short application.\n",
      "* We DO NOT resell or disseminate your email address. You are NOT\n",
      "re ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "url_extractor = None\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'number': 30, 'the': 20, 'you': 14, 'i': 14, 'in': 12, 'to': 11, 'thi': 11, 'and': 11, 'a': 7, 'your': 7, 'are': 6, 'day': 6, 't': 5, 'for': 5, 'is': 5, 'over': 5, 'of': 5, 'don': 4, 'with': 4, 'be': 4, 'we': 4, 'receiv': 4, 'call': 3, 'or': 3, 's': 3, 'fl': 3, 'not': 3, 'have': 3, 'by': 3, 'email': 3, 'want': 2, 'get': 2, 'player': 2, 'on': 2, 'one': 2, 'again': 2, 'do': 2, 'first': 2, 've': 2, 'last': 2, 'sent': 2, 'my': 2, 'back': 2, 'l': 2, 'money': 2, 'c': 2, 'me': 2, 'free': 2, 'lead': 2, 'train': 2, 'will': 2, 'fax': 2, 'immedi': 2, 'mail': 2, 'send': 2, 'remov': 2, 'subject': 2, 'from': 2, 'powerhous': 1, 'gift': 1, 'program': 1, 'miss': 1, 'founder': 1, 'major': 1, 'onc': 1, 'where': 1, 'privat': 1, 'invit': 1, 'expert': 1, 'fastest': 1, 'way': 1, 'huge': 1, 'cash': 1, 'flow': 1, 'ever': 1, 'conceiv': 1, 'leverag': 1, 'into': 1, 'question': 1, 'here': 1, 'either': 1, 'wealthi': 1, 'which': 1, 'am': 1, 'toss': 1, 'financi': 1, 'lifelin': 1, 'sake': 1, 'hope': 1, 'grab': 1, 'onto': 1, 'it': 1, 'hold': 1, 'tight': 1, 'ride': 1, 'life': 1, 'testimoni': 1, 'hear': 1, 'what': 1, 'averag': 1, 'peopl': 1, 'their': 1, 'few': 1, 'that': 1, 'q': 1, 'al': 1, 'm': 1, 'singl': 1, 'mother': 1, 'd': 1, 'wa': 1, 'sure': 1, 'about': 1, 'when': 1, 'off': 1, 'pledg': 1, 'but': 1, 'got': 1, 'veri': 1, 'next': 1, 'ky': 1, 'didn': 1, 'so': 1, 'found': 1, 'myself': 1, 'partner': 1, 'work': 1, 'think': 1, 'made': 1, 'right': 1, 'decis': 1, 'k': 1, 'pick': 1, 'up': 1, 'they': 1, 'gave': 1, 'all': 1, 'can': 1, 'too': 1, 'j': 1, 'w': 1, 'ca': 1, 'announc': 1, 'close': 1, 'sale': 1, 'help': 1, 'blast': 1, 'upon': 1, 'entri': 1, 'make': 1, 'wait': 1, 'now': 1, 'name__________________________________phone___________________________________________': 1, 'fax_____________________________________email____________________________________________': 1, 'best': 1, 'time': 1, 'call_________________________tim': 1, 'zone________________________________________': 1, 'messag': 1, 'complianc': 1, 'new': 1, 'e': 1, 'bill': 1, 'per': 1, 'section': 1, 'paragraph': 1, 'further': 1, 'transmiss': 1, 'sender': 1, 'may': 1, 'stop': 1, 'at': 1, 'no': 1, 'cost': 1, 'repli': 1, 'address': 1, 'word': 1, 'line': 1, 'error': 1, 'omiss': 1, 'except': 1, 'exclud': 1, 'spam': 1, 'compil': 1, 'list': 1, 'our': 1, 'replic': 1, 'databas': 1, 'rel': 1, 'seattl': 1, 'market': 1, 'group': 1, 'gigt': 1, 'turbo': 1, 'team': 1, 'sole': 1, 'purpos': 1, 'these': 1, 'commun': 1, 'continu': 1, 'inclus': 1, 'onli': 1, 'graciou': 1, 'permiss': 1, 'if': 1, 'wish': 1, 'pleas': 1, 'an': 1, 'tesrewint': 1, 'yahoo': 1, 'com': 1, 'delet': 1}),\n",
       "       Counter({'number': 5, 'list': 5, 'the': 4, 'i': 4, 'for': 3, 'mplayer': 3, 'axel': 3, 'rpm': 3, 'to': 2, 'use': 2, 'them': 2, 'xnumber': 2, 'architectur': 2, 'on': 2, 'thimm': 2, 'or': 2, 'reason': 2, 'codec': 2, 'freshrpm': 2, 'net': 2, 'hi': 1, 'ha': 1, 'anyon': 1, 'an': 1, 'answer': 1, 'me': 1, 'document': 1, 'still': 1, 'suggest': 1, 'thank': 1, 'regard': 1, 'sat': 1, 'jun': 1, 'at': 1, 'numberpm': 1, 'wrote': 1, 'are': 1, 'there': 1, 'perhap': 1, 'licens': 1, 'issu': 1, 'which': 1, 'forbid': 1, 'such': 1, 'a': 1, 'practic': 1, 'ani': 1, 'other': 1, 'ask': 1, 'is': 1, 'that': 1, 'have': 1, 'seen': 1, 'perform': 1, 'and': 1, 'also': 1, 'some': 1, 'featur': 1, 'differ': 1, 'fullscreen': 1, 'w': 1, 'o': 1, 'keep': 1, 'aspect': 1, 'ratio': 1, 'visual': 1, 'artifact': 1, 'nvidia': 1, 'compar': 1, 'with': 1, 'without': 1, 'winnumb': 1, 'author': 1, 'seem': 1, 'recommend': 1, 'bewar': 1, 'am': 1, 'no': 1, 'whatev': 1, 'expert': 1, 'may': 1, 'be': 1, 'total': 1, 'lost': 1, 'physik': 1, 'fu': 1, 'berlin': 1, 'de': 1, '_______________________________________________': 1, 'mail': 1, 'http': 1, 'mailman': 1, 'listinfo': 1}),\n",
       "       Counter({'the': 17, 'to': 13, 'it': 10, 'you': 7, 'that': 7, 'be': 7, 'is': 6, 'of': 5, 'a': 5, 'and': 5, 'like': 4, 'more': 4, 'about': 4, 'but': 4, 'so': 4, 'do': 3, 'mean': 3, 'by': 3, 'wa': 3, 'no': 3, 'use': 3, 'product': 3, 'at': 3, 'softwar': 3, 'hardwar': 3, 'requir': 3, 't': 3, 'make': 3, 'crack': 3, 'what': 2, 'cd': 2, 'key': 2, 'or': 2, 'typo': 2, 'unbreak': 2, 'one': 2, 'could': 2, 'system': 2, 'far': 2, 'which': 2, 'an': 2, 'work': 2, 'number': 2, 'mass': 2, 'are': 2, 'look': 2, 'can': 2, 'realli': 2, 'worth': 2, 'in': 2, 'isn': 2, 'come': 2, 'down': 2, 'cracker': 2, 'actual': 2, 'just': 2, 'concern': 2, 'will': 2, 'revers': 2, 'engin': 2, 'glynn': 2, 'clement': 2, 'yannick': 1, 'gingra': 1, 'wrote': 1, 'i': 1, 'presum': 1, 'would': 1, 'here': 1, 'even': 1, 'previous': 1, 'authoris': 1, 'entiti': 1, 'without': 1, 'pay': 1, 'period': 1, 'subscript': 1, 'fee': 1, 'need': 1, 'explicit': 1, 'problem': 1, 'wish': 1, 'solv': 1, 'constraint': 1, 'involv': 1, 'onlin': 1, 'offlin': 1, 'poll': 1, 'frequent': 1, 'offsit': 1, 'server': 1, 'mayb': 1, 'personalis': 1, 'd': 1, 'l': 1, 'sun': 1, 'jdk': 1, 'noth': 1, 'fix': 1, 'yet': 1, 'we': 1, 'way': 1, 'protect': 1, 'from': 1, 'unauthor': 1, 'utilis': 1, 'trust': 1, 'answer': 1, 'fairli': 1, 'complet': 1, 'knowledg': 1, 'busi': 1, 'model': 1, 'all': 1, 'probabl': 1, 'usual': 1, 'how': 1, 'difficult': 1, 'want': 1, 's': 1, 'job': 1, 'if': 1, 'occasion': 1, 'authent': 1, 'simpl': 1, 'copi': 1, 'won': 1, 'ha': 1, 'case': 1, 'issu': 1, 'whether': 1, 're': 1, 'go': 1, 'enter': 1, 'into': 1, 'battl': 1, 'with': 1, 'sure': 1, 'trivial': 1, 'lot': 1, 'your': 1, 'custom': 1, 'base': 1, 'teenag': 1, 'kid': 1, 'tend': 1, 'cost': 1, 'less': 1, 'virus': 1, 'trojan': 1, 'warez': 1, 'fortun': 1, 'corpor': 1, 'view': 1, 'matter': 1, 'differ': 1, 'doe': 1, 'secur': 1, 'ye': 1, 'techniqu': 1, 'onli': 1, 'get': 1, 'same': 1, 'ultim': 1, 'true': 1, 'for': 1, 'resourc': 1, 'other': 1, 'than': 1, 'labour': 1, 'almost': 1, 'anyth': 1, 'may': 1, 'possibl': 1, 'ensur': 1, 'uneconom': 1, 'dvd': 1, 'iirc': 1, 'css': 1, 'player': 1, 'where': 1, 'develop': 1, 'forgot': 1, 'encrypt': 1, 'decrypt': 1, 'virgin': 1, 'net': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 31 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[300,  20,  11,  14,  30,  11,  14,   7,  12,   4,   5],\n",
       "       [108,   4,   2,   0,   5,   1,   4,   1,   0,   1,   1],\n",
       "       [270,  17,  13,   7,   2,   5,   1,   5,   2,   7,   6]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('email_to_wordcount',\n",
       "                 EmailToWordCounterTransformer(lower_case=True,\n",
       "                                               remove_punctuation=True,\n",
       "                                               replace_numbers=True,\n",
       "                                               replace_urls=True, stemming=True,\n",
       "                                               strip_headers=True)),\n",
       "                ('wordcount_to_vector',\n",
       "                 WordCounterToVectorTransformer(vocabulary_size=1000))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.987, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.986, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.992, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9883189803029482"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, precision_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x1001 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 288 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_clf = LogisticRegression(max_iter=1000)\n",
    "log_clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sfsf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ea22aa6e581d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msfsf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sfsf' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
